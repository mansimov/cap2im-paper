\documentclass{article} % For LaTeX2e
\usepackage{iclr-style/iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}

\newcommand{\sigmoid}{\boldsymbol{\sigma}}
\newcommand{\hlang}{h^{lang}}
\newcommand{\hlangall}{\boldsymbol{h^{lang}}}
\newcommand{\hdec}{h^{dec}}
\newcommand{\henc}{h^{enc}}
\newcommand{\readop}{\mathit{read}}
\newcommand{\writeop}{\mathit{write}}
\newcommand{\encoder}{\mathit{LSTM}^{enc}}
\newcommand{\decoder}{\mathit{LSTM}^{dec}}
\newcommand{\canv}{c}
\newcommand{\lat}{z}
\newcommand{\Lat}{Z}
\newcommand{\post}{Q}
\newcommand{\prior}{P}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\lloss}{\mathcal{L}^{z}}
\newcommand{\rloss}{\mathcal{L}^{x}}

\title{Generating Images From Captions\\ With Attention}

\author{
Elman Mansimov\textsuperscript{*}, Emilio Parisotto\thanks{Equal Contribution} , Jimmy Lei Ba \& Ruslan Salakhutdinov\\
Department of Computer Science\\
University of Toronto\\
Toronto, Ontario, Canada \\
\texttt{\{emansim,eparisotto,rsalakhu\}@cs.toronto.edu}, \texttt{jimmy@psi.utoronto.ca}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
At the end.
\end{abstract}

\section{Introduction}

Generating realistic images from their descriptions is a very hard task that combines together two challenging problems of language modelling and image generation. The model has to capture the semantic knowledge expressed in the description and then use it to generate the values of three color channels of each pixel in the image. Generating images from captions is a significantly harder problem than a reverse problem of generating captions from images which has recenly attracted a lot of attention.

\section{Related Work}

Deep Neural Networks have achieved a remarkable performance in various tasks such as image recognition \citep{krizhevsky_imagenet}, speech transcription \citep{graves_speech} and etc. While most of the recent success has been achieved by discriminative models, the generative models have not yet enjoyed the same level of success. Most of the previous work in generative models has been focused on variants of Boltzmann Machines \citep{smolensky_rbm}, \citep{russ_dbm} and Deep Belief Networks \citep{hinton_dbn}. While these models are very powerful, each iteration of training requires a computationally costly step of MCMC to approximate an intractable normalization constant that makes it hard to scale them to large datasets.

\cite{kingma_vae} have introduced the Variational Auto-Encoder (VAE) which can be seen as a neural network with continous latent variables. The encoder is used to approximate posterior distribution and the decoder is used to stochastically reconstruct the data from latent variables. The model performs an efficient inference and learning that allows it to scale to large datasets. \cite{gregor_draw} have introduced Deep Recurrent Attention Writer (DRAW), where they have incorporated a novel differentiable attention mechanism into the VAE which significantly improved its performance as well as quality of generated samples. While most of the samples of VAE and DRAW resemble a clear structure of objects, the generated images are blurry most of the time.

Generative Adversarial Networks (GANs) \citep{goodfellow_gan} are another type of generative models that use noise-contrastive estimation \citep{gutmann_nce} to avoid calculating intractable normalization constant. The model consists of generator that generates samples from uniform distribution and discriminator that discriminates between real and generated images. Both networks are playing the game, where generator tries to produce samples that look real and discriminator tries not to be fooled by generator. Recently, \cite{denton_lapgan} have scaled those models, by training GANs at each level of Laplacian pyramid of images. While their model has generated sharp looking samples, the generated images have lacked a clear structure. Compared to other mentioned generative models, GANs are more unstable and are harder to train.

While all of the previous work has been focused on unconditional models or models conditioned on labels, to the best of our knowledge this paper is the first to introduce the generative model of images conditioned on captions.

\section{Model}

Our proposed model can be seen as a part of sequence-to-sequence framework \citep{ilya_mt}, \citep{cho_mt}, \citep{nitish_video} where captions are represented as a sequence of consecutive words and images are represented as a sequence of patches drawn on canvas over time $t=1,...,T$. Let $y$ be the input caption, consisting of $N$ words $y_{1}, y_{2}, ..., y_{n}$ and $x$ be the image corresponding to that caption.

\subsection{Encoder}

The encoder is a deterministic Bidirectional LSTM that encodes the variable size sentences into the vector representation $s$. Bidirectional LSTM consists of one Forward LSTM and Backward LSTM which combine information from past and future respectively. The Forward LSTM computes the sequence of forward hidden states $[\overrightarrow{h}^{lang}_{1}, \overrightarrow{h}^{lang}_{2}, ..., \overrightarrow{h}^{lang}_{N}]$ , whereas the Backward LSTM computes the sequence of backward hidden states $[\overleftarrow{h}^{lang}_{1}, \overleftarrow{h}^{lang}_{2}, ..., \overleftarrow{h}^{lang}_{N}]$. Then these hidden states are concatenated together into the sequence $[\hlang_{1}, \hlang_{2}, ..., \hlang_{N}]$, where $\hlang_{n} = [\overrightarrow{h}^{lang}_{n}, \overleftarrow{h}^{lang}_{n}], 1\leq n\leq N$.

The final representation of the sentence is calculated as follows:
\begin{align}
s_{t} = \alpha_{1}\hlang_{1} + \alpha_{2}\hlang_{2} + ... + \alpha_{N}\hlang_{N}
\end{align}
where $\hlang_{0}$ is initialized to the learned bias.
Setting $\alpha_{1...N}$ to $\frac{1}{N}$ turns the encoder into the vanilla model introduced in \citep{cho_mt} without the attention. We will describe how $\alpha_{1...N}$ are calculated in the next section.

\subsection{Decoder}

The decoder is a DRAW model, which is a stochastic recurrent neural network that consists of Inference LSTM that infers the distribution of latent variables of image $x$ given $y$ and then the Generator LSTM that uses the inferred latent variables in order to reconstruct the image $x$ given $y$. Formally, the decoder iteratively computes the following equations for $t=1,...,T$
\begin{align}
\label{eq:x_hat}
\hat{x}_t &= x-\sigmoid(\canv_{t-1})\\
\label{eq:read}
r_t &= \readop(x_t, \hat{x}_t, \hdec_{t-1})\\
\henc_t &= \encoder(\henc_{t-1}, [r_t, \hdec_{t-1}])\\
\lat_t &\sim \post(\Lat_t|\henc_t)\\
\hdec_t &= \decoder(\hdec_{t-1}, z_t, s_{t-1})\\
s_{t} &= align(\hdec_{t-1}, \hlangall)\\
\label{eq:write}
\canv_t &= \canv_{t-1} + \writeop(\hdec_t)
\end{align}

where $\readop$ and $\writeop$ are the same attention operators as in \citep{gregor_draw}, $\hlangall = [\hlang_{1}, \hlang_{2}, ..., \hlang_{n}]$ and $\canv_{0}, \hdec_{0}, \henc_{0}$ are initialized to the learned biases.

The $align$ function introduced by \cite{bahdanau_mt} is used to compute probabilities $\alpha_{1...n}$ by first computing the energies $e_{t,0}, e_{t,1}, ..., e_{t,n}$ and then rescaling them to the probability distribution $\alpha_{1}, \alpha_{2}, ..., \alpha_{n}$
\begin{align}
e_{tj} &= v^{T}tanh(U\hlang_{j} + W\hdec_{t} + b)\\
\alpha_{j} &= softmax(e_{tj})
\end{align}

where $softmax(e_{tj}) = \frac{exp(e_{tj})}{\sum_{j=1}^{N}exp(e_{tj})}$

\subsection{Learning}

The model is learned by the modified version of Stochastic Gradient Variation Bayes (SGVB) algorithm introduced by \cite{kingma_vae}. The model is trained to maximize the lower bound of marginal likelihood $\loss$ of the correct image $x$ given the input caption $y$. The $\loss$ is decomposed into the latent loss $\lloss$ and the reconstruction loss $\rloss$. 

The reconstruction loss $\rloss$ equals to $\frac{1}{L}\sum_{l=1}^{L}(log\,p(x_{t}|y,z)$ where $L$ is the number of samples used during training, which was set to $1$ in our experiments.

The latent loss is a negative sum of Kullback--Leibler divergence terms between distribution $\post(\Lat_t|\henc_t)$ and some prior distribution ${\prior(\Lat_t)}$ over time $t=1,...,T$, which can be seen as a regularization term. Since the patches drawn on canvas over time are not independent of each other, naturally the sufficient statistics of the prior distribution at time $t$ should be dependent on the sufficient statistics of the prior distribution at time $t-1$. Therefore, instead of setting $\prior(\Lat_1), ..., \prior(\Lat_T)$ to be independent unit gaussian distributions, the mean and variance of $\prior(\Lat_t)$ depends on the $\hdec_{t-1}$, as in \citep{bachman_sdm}, where 
\begin{align}
\mu_{t}^{prior} &= tanh(W_{mu}\hdec_{t-1})\\
\sigma_{t}^{prior} &= exp(tanh(W_{\sigma}\hdec_{t-1})) 
\end{align}
Overall, the likelihood $\loss$ is calculated as follows:
\begin{align}
\loss &= -\sum_{t=1}^{T}D_{KL}(\post(\Lat_t|\henc_t,s_{t-1})\,||\,\prior(\Lat_t)) + \frac{1}{L}\sum_{l=1}^{L}log\,p(x_{t}|y,z)\\
&=
\frac{1}{2}\sum_{t=1}^{T}(1 - 2\,log\,\sigma_{t}^{prior} + 2\,log\,\sigma_{t} - \frac{exp(2\,log\,\sigma_{t}) + (\mu_{t} - \mu_{t}^{prior})^{2}}{exp(2\,log\,\sigma_{t}^{prior})}) + \frac{1}{L}\sum_{l=1}^{L}log\,p(x_{t}|y,z)
\end{align}

\subsection{Image Generation}

During the image generation step, we throw away the Inference LSTM from the Decoder and instead sample from the prior distribution. Due to the samples from DRAW model being blurry, we do an additional post processing step, where we use adversarial net ...

%\section{Model}

%Our proposed model can be seen as a part of sequence-to-sequence framework \citep{ilya_mt}, \citep{cho_mt}, \citep{nitish_video} where captions are represented as a sequence of consecutive words and images are represented as a sequence of patches drawn on canvas over time. The encoder is a RNNSearch model \citep{bahdanau_mt}, which is a deterministic recurrent neural network that encodes the variable sized sentence into the vector representation. The decoder is a DRAW model, which is a stochastic recurrent neural network that consists of inference network that infers the distribution of latent variables of images and then the generator network uses the inferred latent variables in order to reconstruct the images. At the test time, the images are sampled from rr distribution given the vector representation of the caption.

%A soft attention mechanism in the encoder combines the hidden states of the rnn as a weighted sum, where the weights are updated at every timestep depending on the hidden states of the encoder and the hidden states of the generator network in the decoder. The dynamically updated vector representation of the captions helps the model to cope with long sentences and achieve a better performance, without using tricks such as reversing the source sentences. A selective attention mechanism in the decoder restricts both the input region obverved by inference network and the output region modified by the generator network. 

%The use of attention in both the encoder and the decoder allows the model to generate the parts of image while focusing the relevant representations of words in the caption.

%Mathematically, the model is trained by maximizing the marginal likelihood $L$ of a correct image $x$ given the caption $y$, consisting of words $y_{1}, y_{2}, ..., y_{n}$. Marginal likelihood is defined as 
%$L = -\sum_{t=1}^{T}D_{KL}(q(z_{t}|x,y) || p(z_{t})) + \frac{1}{s}\sum_{s=1}^{S}(logp(x|y,z)$, where $s$ is the number of samples which was $1$ during training.

%Due to generated images from DRAW being blurry, as a post processing step we have used ...

\section{Experiments}

\bibliography{iclr-paper}
\bibliographystyle{iclr-style/iclr2016_conference}

\end{document}